
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Project\n",
    "## üìÅ Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßº Step 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['cp', 'thal', 'slope'], drop_first=True)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Step 5: Train-Test Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_encoded.drop('target', axis=1)\n",
    "y = df_encoded['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Model Training (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "lr_model = LogisticRegression().fit(X_train, y_train)\n",
    "svm_model = SVC(probability=True).fit(X_train, y_train)\n",
    "knn_model = KNeighborsClassifier().fit(X_train, y_train)\n",
    "dt_model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "rf_model = RandomForestClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 7: Evaluation Function with Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    ap = average_precision_score(y_test, y_probs)\n",
    "    auc = roc_auc_score(y_test, y_probs)\n",
    "    print(f"\U0001F4CC {name} Metrics:")\n",
    "    print(f"Accuracy Score: {acc:.4f}")\n",
    "    print(f"Average Precision Score: {ap:.4f}")\n",
    "    print(f"AUC Score: {auc:.4f}")\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f'{name} AUC = {auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f'ROC Curve - {name}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f'{name} AP = {ap:.2f}', color='orange')\n",
    "    plt.title(f'Precision-Recall Curve - {name}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return {"Model": name, "Accuracy": acc, "Average Precision": ap, "AUC": auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 8: Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results.append(evaluate_model("Logistic Regression", lr_model, X_test, y_test))\n",
    "results.append(evaluate_model("SVM", svm_model, X_test, y_test))\n",
    "results.append(evaluate_model("KNN", knn_model, X_test, y_test))\n",
    "results.append(evaluate_model("Decision Tree", dt_model, X_test, y_test))\n",
    "results.append(evaluate_model("Random Forest", rf_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Line Chart Comparison of All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df["Model"], results_df["Accuracy"], label="Accuracy", marker='o')\n",
    "plt.plot(results_df["Model"], results_df["Average Precision"], label="Avg Precision", marker='o')\n",
    "plt.plot(results_df["Model"], results_df["AUC"], label="AUC", marker='o')\n",
    "plt.title("Model Performance Comparison")\n",
    "plt.xlabel("Model")\n",
    "plt.ylabel("Score")\n",
    "plt.ylim(0.6, 1.05)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
